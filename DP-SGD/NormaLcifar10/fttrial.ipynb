{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyMGK4Jrkr2g8ZlLhFWJz0H2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtMgYty2eu2l","executionInfo":{"status":"ok","timestamp":1701744551602,"user_tz":300,"elapsed":8465,"user":{"displayName":"jingtian ke","userId":"11426208050859465028"}},"outputId":"2e1b7fd4-7bb5-4fe0-c17e-77ba85ff423e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/awslabs/fast-differential-privacy\n","  Cloning https://github.com/awslabs/fast-differential-privacy to /tmp/pip-req-build-n2wccy4a\n","  Running command git clone --filter=blob:none --quiet https://github.com/awslabs/fast-differential-privacy /tmp/pip-req-build-n2wccy4a\n","  Resolved https://github.com/awslabs/fast-differential-privacy to commit 1202baa441c38c338415bc24e4cf12514c46bac0\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: fastDP\n","  Building wheel for fastDP (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fastDP: filename=fastDP-1.1.0-py3-none-any.whl size=101664 sha256=cf1accc3de7649cf3b54276f4dca95602df77255c0bfc4cec9c3fb084915364f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-e8612w00/wheels/b3/2b/b5/62668a08336b29e28ca790358738474aa8c2c68359b494f454\n","Successfully built fastDP\n","Installing collected packages: fastDP\n","Successfully installed fastDP-1.1.0\n"]}],"source":["%pip install git+https://github.com/awslabs/fast-differential-privacy"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"s4rxv_cQfKGs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701744575065,"user_tz":300,"elapsed":23469,"user":{"displayName":"jingtian ke","userId":"11426208050859465028"}},"outputId":"4e848653-6d88-4071-c0ad-1dd956d3d3a1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/Othercomputers/My Mac/NormaLcifar10/"],"metadata":{"id":"TC4H4xoHfLR0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701744576064,"user_tz":300,"elapsed":1005,"user":{"displayName":"jingtian ke","userId":"11426208050859465028"}},"outputId":"2afc8fbe-4444-4f08-96df-0792869f8a96"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Othercomputers/My Mac/NormaLcifar10\n"]}]},{"cell_type":"code","source":["!python finetuning.py --name fttrial --seed 0 --epoch 50 --epsilon 1 --b 100 --lr 1 --gamma 0.8"],"metadata":{"id":"wm3Twvq1fNrO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ebccc172-8c9c-40a5-c212-55337ba87650"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100% 44.7M/44.7M [00:00<00:00, 67.5MB/s]\n","Number of trainable components:  22 ; Number of trainable layers:  21\n",">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['conv1']\n","noise multiplier 1.4625244140625\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n","['bn1.weight', 'bn1.bias', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias'] are not supported by privacy engine; these parameters are not requiring gradient nor updated.\n","[1,   200] loss: 0.000\n","[1,   400] loss: 0.000\n","/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:129: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Seems like `optimizer.step()` has been overridden after learning rate scheduler \"\n","Accuracy of the network on the 10000 test images: 10 %\n","[2,   200] loss: 0.000\n","[2,   400] loss: 0.000\n","Accuracy of the network on the 10000 test images: 12 %\n","[3,   200] loss: 0.000\n","[3,   400] loss: 0.000\n","Accuracy of the network on the 10000 test images: 12 %\n","[4,   200] loss: 0.000\n","[4,   400] loss: 0.000\n","Accuracy of the network on the 10000 test images: 12 %\n","[5,   200] loss: 0.000\n","[5,   400] loss: 0.000\n","Accuracy of the network on the 10000 test images: 13 %\n","[6,   200] loss: 0.000\n","[6,   400] loss: 0.000\n","Accuracy of the network on the 10000 test images: 12 %\n","[7,   200] loss: 0.000\n","[7,   400] loss: 0.000\n","Accuracy of the network on the 10000 test images: 12 %\n","[8,   200] loss: 0.000\n","[8,   400] loss: 0.000\n","Accuracy of the network on the 10000 test images: 12 %\n","[9,   200] loss: 0.000\n","[9,   400] loss: 0.000\n","Accuracy of the network on the 10000 test images: 13 %\n","[10,   200] loss: 0.000\n","[10,   400] loss: 0.000\n","Accuracy of the network on the 10000 test images: 13 %\n"]}]},{"cell_type":"markdown","source":["- !python finetuning.py --name fttrial --seed 0 --epoch 50 --epsilon 1.5 --b 100 --lr 0.01\n","\n","  0 %,1 %,4 %,7 %,13 %,21 %,28 %,35 %,40 %,44 %,47 %,48 %,49 %,49 %,47 %,44 %,43 %\n","\n","  noise multiplier 1.1159423828125\n","\n","- !python finetuning.py --name fttrial --seed 0 --epoch 50 --epsilon 1 --b 100 --lr 0.01\n","\n","  noise multiplier 1.4625244140625"],"metadata":{"id":"oAvcemDb2I0j"}}]}