{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNuyEmVHztc2Gf91XWan/cw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DtMgYty2eu2l","outputId":"f3145f83-ec2c-4c4f-ae2a-5dec930a014e","executionInfo":{"status":"ok","timestamp":1701753357698,"user_tz":300,"elapsed":8220,"user":{"displayName":"jingtian ke","userId":"11426208050859465028"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/awslabs/fast-differential-privacy\n","  Cloning https://github.com/awslabs/fast-differential-privacy to /tmp/pip-req-build-of05vek6\n","  Running command git clone --filter=blob:none --quiet https://github.com/awslabs/fast-differential-privacy /tmp/pip-req-build-of05vek6\n","  Resolved https://github.com/awslabs/fast-differential-privacy to commit 1202baa441c38c338415bc24e4cf12514c46bac0\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: fastDP\n","  Building wheel for fastDP (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fastDP: filename=fastDP-1.1.0-py3-none-any.whl size=101664 sha256=674f888c6634f806c2388c695b963eadf5924307546cadda425d0ac14ca716b9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ej9pck2l/wheels/b3/2b/b5/62668a08336b29e28ca790358738474aa8c2c68359b494f454\n","Successfully built fastDP\n","Installing collected packages: fastDP\n","Successfully installed fastDP-1.1.0\n"]}],"source":["%pip install git+https://github.com/awslabs/fast-differential-privacy"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"s4rxv_cQfKGs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701753377237,"user_tz":300,"elapsed":19541,"user":{"displayName":"jingtian ke","userId":"11426208050859465028"}},"outputId":"918de58e-1d99-46f3-a96b-2e808fb22948"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/Othercomputers/My Mac/NormaLcifar10/"],"metadata":{"id":"TC4H4xoHfLR0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701753377978,"user_tz":300,"elapsed":747,"user":{"displayName":"jingtian ke","userId":"11426208050859465028"}},"outputId":"c7f2769e-6433-449f-cc8b-da84a7bb50eb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/Othercomputers/My Mac/NormaLcifar10\n"]}]},{"cell_type":"code","source":["!python finetuning.py --name fttrial --seed 0 --epoch 20 --epsilon 1 --b 64 --lr 0.1"],"metadata":{"id":"wm3Twvq1fNrO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f884e82b-3657-485a-940a-e27ee8dee04e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100% 44.7M/44.7M [00:00<00:00, 158MB/s]\n","Number of trainable components:  22 ; Number of trainable layers:  21\n",">>>>>>>>>>>>>>>>> Applying  automatic  per-sample gradient clipping.\n",">>>>>>>>>>>>>>>>> Block heads for per-sample gradient clipping are defined as: ['conv1']\n","noise multiplier 1.0093017578125\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n","['bn1.weight', 'bn1.bias', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias'] are not supported by privacy engine; these parameters are not requiring gradient nor updated.\n"]}]},{"cell_type":"markdown","source":["- !python finetuning.py --name fttrial --seed 0 --epoch 50 --epsilon 1.5 --b 100 --lr 0.01\n","\n","  0 %,1 %,4 %,7 %,13 %,21 %,28 %,35 %,40 %,44 %,47 %,48 %,49 %,49 %,47 %,44 %,43 %\n","\n","  noise multiplier 1.1159423828125\n","\n","- !python finetuning.py --name fttrial --seed 0 --epoch 50 --epsilon 1 --b 100 --lr 0.01\n","\n","  noise multiplier 1.4625244140625\n","\n","- !python finetuning.py --name fttrial --seed 0 --epoch 50 --epsilon 1 --b 100 --lr 1 --gamma 0.8\n","\n","  10 %,12 %,12 %,12 %,13 %,12 %,12 %,12 %,13 %,13 %,13 %,13 %\n","\n","- !python finetuning.py --name fttrial --seed 0 --epoch 50 --epsilon 1.5 --b 32 --lr 0.01\n","\n","  5 %,27 %,36 %,38 %,36 %,30 %,15 %,20 %,20 %,20 %"],"metadata":{"id":"oAvcemDb2I0j"}}]}